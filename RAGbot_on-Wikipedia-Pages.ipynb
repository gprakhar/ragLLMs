{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RJ2d86KccFIR"
      },
      "outputs": [],
      "source": [
        "!pip install -U --quiet langchain langchain_community langchain-google-vertexai langchain_chroma\n",
        "!pip install --quiet \"unstructured[all-docs]\" pypdf pillow pydantic lxml pillow chromadb tiktoken wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = \"\"\n",
        "REGION = \"\"\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "IuQAB5gddzYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import vertexai\n",
        "vertexai.init(project = PROJECT_ID , location = REGION)"
      ],
      "metadata": {
        "id": "jfva4TlKendO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download and load wikipedia page data\n",
        "import logging\n",
        "from langchain_community.document_loaders import WikipediaLoader\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "docs = []\n",
        "#Query Wikipedia and load the articles\n",
        "for item in [\"Solar System\", \"Sun\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\", \"Venus\", \"Mercury planet\", \"Asteroid\", \"comet\"]:\n",
        "    loader = WikipediaLoader(query=item, load_max_docs=1, doc_content_chars_max=100000000)\n",
        "    docs.append(loader.load())\n"
      ],
      "metadata": {
        "id": "tcSJmggUeu9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Put page content into this list\n",
        "texts = [d[0].page_content for d in docs if d]\n",
        "list_of_titles = [d[0].metadata.get(\"title\") for d in docs if d]"
      ],
      "metadata": {
        "id": "qCp-4qLIe6iS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Text summaries\n",
        "from langchain_google_vertexai import VertexAI , ChatVertexAI , VertexAIEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda"
      ],
      "metadata": {
        "id": "L_OkhJydfpHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read about Runnable Lambda here - https://www.pinecone.io/learn/series/langchain/langchain-expression-language/"
      ],
      "metadata": {
        "id": "Fp-Hq2mWiX5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate summaries of text elements\n",
        "def generate_text_summaries(texts, summarize_texts=False):\n",
        "   \"\"\"\n",
        "   Summarize text elements\n",
        "   texts: List of str\n",
        "   summarize_texts: Bool to summarize texts\n",
        "   \"\"\"\n",
        "\n",
        "   # Prompt\n",
        "   prompt_text = \"\"\"You are an assistant tasked with summarizing text for retrieval. \\\n",
        "   These summaries will be embedded and used to retrieve the raw text. \\\n",
        "   Give a concise summary of the text that is well optimized for retrieval. text: {element} \"\"\"\n",
        "   prompt = PromptTemplate.from_template(prompt_text)\n",
        "   empty_response = RunnableLambda(\n",
        "       lambda x: AIMessage(content=\"Error processing document\")\n",
        "   )\n",
        "\n",
        "   # Text summary chain\n",
        "   model = VertexAI(\n",
        "       temperature=0, model_name=\"gemini-pro\", max_output_tokens=1024\n",
        "   ).with_fallbacks([empty_response])\n",
        "\n",
        "   summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
        "\n",
        "   # Initialize empty summaries\n",
        "   text_summaries = []\n",
        "\n",
        "   # Apply to text if texts are provided and summarization is requested\n",
        "   if texts and summarize_texts:\n",
        "       text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 2})\n",
        "   elif texts:\n",
        "       text_summaries = texts\n",
        "\n",
        "   return(text_summaries)\n",
        "\n",
        "\n",
        "# Get text summaries\n",
        "text_summaries = generate_text_summaries(texts, summarize_texts=True)"
      ],
      "metadata": {
        "id": "noAOMgjXxVKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for multi vector Retrival"
      ],
      "metadata": {
        "id": "Aon-0GZascTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document"
      ],
      "metadata": {
        "id": "JVjkbcnlskZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This function provides functionality to first setup a retrival funtion,\n",
        "# then function to add the supplied documents to the DB (both the summary and\n",
        "# raw text), then return this initialized Retriver function\n",
        "def create_multi_vector_retriever(\n",
        "   vectorstore, text_summaries, texts):\n",
        "   \"\"\"\n",
        "   Create retriever that indexes summaries, but returns raw images or texts\n",
        "   \"\"\"\n",
        "\n",
        "   # Initialize the storage layer\n",
        "   store = InMemoryStore()\n",
        "   id_key = \"doc_id\"\n",
        "\n",
        "   # Create the multi-vector retriever\n",
        "   retriever = MultiVectorRetriever(\n",
        "       vectorstore=vectorstore,\n",
        "       docstore=store,\n",
        "       id_key=id_key,\n",
        "   )\n",
        "\n",
        "   # Helper function to add documents to the vectorstore and docstore\n",
        "   def add_documents(retriever, doc_summaries, doc_contents):\n",
        "       doc_ids = [str(uuid.uuid4()) for _ in doc_contents] #generate uniq ids\n",
        "       summary_docs = [\n",
        "           Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "           for i, s in enumerate(doc_summaries)\n",
        "       ]\n",
        "       retriever.vectorstore.add_documents(summary_docs) # Summary added to DB\n",
        "       retriever.docstore.mset(list(zip(doc_ids, doc_contents)))# Raw text added to DB\n",
        "\n",
        "   # Add texts, tables, and images\n",
        "   # Check that text_summaries is not empty before adding\n",
        "   if text_summaries:\n",
        "       add_documents(retriever, text_summaries, texts)\n",
        "\n",
        "   return(retriever)\n",
        "\n",
        "\n",
        "# The vectorstore to use to Vecterize, Embbed and index the summaries\n",
        "vectorstore = Chroma(\n",
        "   collection_name=\"test-RAG-WikiPages\",\n",
        "   embedding_function=VertexAIEmbeddings(model_name=\"textembedding-gecko@latest\"),\n",
        ")\n",
        "\n",
        "# Create and initialize retriever using the functions created above\n",
        "retriever_multi_vector_img = create_multi_vector_retriever(\n",
        "   vectorstore,\n",
        "   text_summaries,\n",
        "   texts\n",
        ")\n"
      ],
      "metadata": {
        "id": "NhH4nII9smqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the milti modal RAG pipeline"
      ],
      "metadata": {
        "id": "AfFi2UMuyuet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_core.messages import HumanMessage\n",
        "import io"
      ],
      "metadata": {
        "id": "nHzRnrZ_y1Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert_Doc_to_Dict() - THis function coverts the retriver output to a dict\n",
        "def convert_Doc_to_Dict(docs):\n",
        "   \"\"\"\n",
        "   coverts the retriver output to a dict\n",
        "   \"\"\"\n",
        "   b64_images = []\n",
        "   texts = []\n",
        "   for doc in docs:\n",
        "       # Check if the document is of type Document and extract page_content if so\n",
        "       if isinstance(doc, Document):\n",
        "           doc = doc.page_content\n",
        "           texts.append(doc)\n",
        "   return({\"images\": b64_images, \"texts\": texts})"
      ],
      "metadata": {
        "id": "35wN-ZgZPeDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This function creates the prompt for the RAG task\n",
        "def rag_prompt_func(data_dict):\n",
        "   \"\"\"\n",
        "   Join the context into a single string\n",
        "   \"\"\"\n",
        "   formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
        "   messages = []\n",
        "\n",
        "   # Adding the text for analysis\n",
        "   text_message = {\n",
        "       \"type\": \"text\",\n",
        "       \"text\": (\n",
        "           \"You are responsible AI assistant who is role playing a middle school science teacher. \\n\"\n",
        "           \"You will be given text. \\n\"\n",
        "           \"You can only Use this information to answer the question from your students. \\n\"\n",
        "           \"If the answer in not part of your context, just say that you don't know, don't try to make up an answer. \\n\\n\"\n",
        "           f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
        "           \"Text and / or tables:\\n\"\n",
        "           f\"{formatted_texts}\"\n",
        "       ),\n",
        "   }\n",
        "   messages.append(text_message)\n",
        "\n",
        "   return([HumanMessage(content=messages)])\n"
      ],
      "metadata": {
        "id": "ReRWqWXfD3oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_modal_rag_chain(retriever):\n",
        "   \"\"\"\n",
        "   Multi-modal RAG chain\n",
        "   \"\"\"\n",
        "\n",
        "   # Multi-modal LLM\n",
        "   model = ChatVertexAI(\n",
        "       temperature=0, model_name=\"gemini-pro\", max_output_tokens=1024\n",
        "       #gemini-1.5-flash, gemma2,\n",
        "   )\n",
        "\n",
        "   # RAG pipeline\n",
        "   chain = (\n",
        "       {\n",
        "           \"context\": retriever | RunnableLambda(convert_Doc_to_Dict),\n",
        "           \"question\": RunnablePassthrough(),\n",
        "       }\n",
        "       | RunnableLambda(rag_prompt_func)\n",
        "       | model\n",
        "       | StrOutputParser()\n",
        "   )\n",
        "\n",
        "   return chain\n",
        "\n",
        "\n",
        "# Create RAG chain\n",
        "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FcY41_mYt4bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Retrival pipeline"
      ],
      "metadata": {
        "id": "EP7BkhdZE3GX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What updates do you have about Genrative AI technology space?\"\n",
        "docs = retriever_multi_vector_img.get_relevant_documents(query, limit=3)\n",
        "\n",
        "# We get relevant docs\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "VQWtQAq4Cdkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling the RAG pipeline"
      ],
      "metadata": {
        "id": "1nn8JikhCW83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the diameter of the pluto?\"\n",
        "result = chain_multimodal_rag.invoke(query)\n",
        "\n",
        "from IPython.display import Markdown as md\n",
        "md(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "A8n6Nl8xVkOk",
        "outputId": "f7db3f7b-8809-4618-8fd5-516e2c71df75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I'm sorry, but the text you provided does not contain information about the diameter of Pluto. Therefore, I cannot answer your question. \n"
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the diameter a one rupee coin?\"\n",
        "result = chain_multimodal_rag.invoke(query)\n",
        "\n",
        "from IPython.display import Markdown as md\n",
        "md(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "c588f2bd-205a-4f9e-9b68-01a30b21e19c",
        "id": "9p_BmUp_Wfkd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I'm sorry, but the text you provided does not contain information about the diameter of a one rupee coin. Therefore, I cannot answer your question. \n\nWould you like me to try to find the answer to your question from another source? \n"
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}
